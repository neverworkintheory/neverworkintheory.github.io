Yes, hi everyone, my name is Zadia Codabux, I'm an assistant professor at the University of Saskatchewan in Canada.
So today I'm going to focus on technical debt in R packages.
Technical debt has been primarily studied in object-oriented system, mostly commercial software in Java,
so in today's talk we're gonna take the path less traveled and focus on R.
More specifically, we're going to explore technical debt in R packages.
So why R?
R has gained popularity over the last few years as we can see in these images.
Most R developers are also not software developers by training,
so they are more versed in their respective domains so here we're talking about geologists, physicians, system bioinformations, etc.
These people eventually became - become the end users of the software that they implement.
And the R ecosystem is relatively young and unexplored,
so this is a great opportunity for us to look at technical debt in R packages from its infancy,
and also understand the nuances in how R developers differ from others.
So in the first part of the talk I'll focus on a study where we examine technical debt,
which is documented in the peer review of R packages in rOpenSci.
So rOpenSci features basically a platform to organize and review R packages,
and it has established a peer review process for packages that are submitted before they are published.
So in this study we explore the technical debt types mentioned in these reviews,
their distribution,
and whether the types of debt highlighted differ based on different user rules.
So by user roles here we refer to authors, reviewers, and editors of these R packages.
So we came up with a taxonomy of 10 different types of debt grouped according to these three perspectives.
So perspective is basically who is affected by a particular type of debt.
So we have three perspectives here: the user, developer, and CRAN.
So CRAN is the Comprehensive R Archive Network which is basically - which basically performs automated checks on all packages.
Here we can see that different user roles focus on different types of debt mentioned in these reviews.
So what we can learn from this is that different categories of people have different perspectives of what technical debt means when it comes to R packages.
So the next set of results here is,
when we look at how often these types of debt come up in the reviews,
we see that documentation debt is basically the most valued.
And this is really interesting because in traditional object-oriented systems,
most studies have shown that code debt - design debt are the most prominent
and developers focused on managing those as a priority
whereas in a growing ecosystem like R documentation is the most important.
So here we look at the different roles and what type of debt do they focus on.
So we can see that if you're an author,
you are mostly focused on defect debt.
However, editors and - editors and reviewers have different priorities.
They focus more on documentation,
which kind of makes sense because they want the R packages to be reused, customized, and extended.
So in the second part of the study - in the second study I'm going to focus on a self-admitted technical debt,
again in R packages.
So,
self-admitted technical debt are basically situations where the developers are aware that
the implementation - the current implementation - is not optimal and they write comments in the source code to alert of such cases.
So in this study we focused on automated detection of SATD using classification models.
So we used a traditional machine learning model and also neural nets including transformer models.
So what we wanted to explore in this study is,
what is the best model for classifying self-admitted technical dept and its types.
And we also investigate the causess of technical debt - the causes leading to the occurrences of SATD in R packages.
However in this talk I'll only focus on the automated detection part.
So we can see a transformer model Alberta and - ALBERT and RoBERTa have a better result when it comes to SATD prediction
so they have better performance overall,
so higher recall higher precision and higher F1 score.
However they are more computationally expensive if we look at the training time compared to other models
And then we have CNN which has a pretty good performance overall both in terms of training time and performance,
so we can say that CNN seems to hit the sweet spot.
However when we look at these models,
when we look at how, you know, these models perform when it comes to a specific type of debt,
we can see that some perform better than others
and as we know the documentation debt is important for R packages from our first study.
We can see that some of these models do not detect a documentation debt
and we can see some of them do not also detect things like people debt.
So here there are some trade-offs which have to be made, you know,
we have to decide what is important when picking a model for automated debt detection.
Is it more important to extract all the different types of debt or is the performance of the model, you know, important?
So even in the best performing model RoBERTa,
we can see that there are some pretty low numbers,
meaning that some types of debt are harder to detect than others.
So there are some takeaways based on these two studies.
So some of - the first one is, documentation is important in R to understand how to get started, how to use the packages,
and this also provides an opportunity to investigate what constitutes good documentation in R,
how to decrease documentation debt,
and also promote the reuse of R packages.
Second, different users focus on different types of debt.
So the end users have different needs and priorities compared to the editors and reviewers.
Lastly,
some types of debt are more challenging to detect.
I'm gonna go over a couple of examples here.
Requirement debt is one of those, you know, challenging types of debt to detect,
and when we investigated,
we saw that the structure the wording - the quality of the comments fluctuated considerably.
And we also know that R packages are used in multiple domains - bioinformatics, geography, etc. - and the way the requirements are documented,
they're also vary considerably.
The next one is algorithm debt.
Algorithm debt was hard to detect because there's not a lot of keywords for detecting this type of debt.
So these are all hard problems and it would be super interesting if we could build a tool to better detect such types of debt.
So thank you for listening.
If you're interested in talking more about these studies or collaborating feel free to reach out.
Fantastic, great job, love it, love it, I mean just all of the really cool things going on here today.
Never really thought explicitly about technical debt in the context of R but this is thought provoking,
because I feel like,
especially with the documentation debt,
I use a lot of our documentation and I found that there's a lot out there,
but it's interesting - I wonder when the documentation, in terms of this work,
is documentation debt internal,
is that kind of what that's referring to or is that on a larger scale?
No we mostly looked at, you know, like, README files and, you know, documentation provided by the authors, you know,
so that the packages could be reused and used by other people, extended.
We also saw that there are a lot of packages which actually does the same thing.
So why are people not reusing, you know, because they don't feel they understand what's going on, right,
so they build something from scratch.
That is interesting, because if you don't understand it then you don't know it exists
and so you make it wasn't doing as well.
How do you use it - the power of documentation, why have we not grasped it?
I love it.
Okay, great any other questions, I think we have time for some more,
okay,
I think that the question came into Slack here from Greg.
Greg wants to know how do people learn how to write decent documentation?
Seems like it's taught even less than debugging or testing.
Plus one on that one.
We have we have ongoing work,
so this is, you know, part of the work that we are doing,
we have ongoing work we where we actually look at, you know, a bunch of documentation,
a bunch of like READMEs and things like that,
and we came up with, like, a taxonomy,
so this is still work in review but, you know, like a checklist and, you know, taxonomies,
what should be included in documentation and so on,
how to make it useful,
so this is you know ongoing work,
so we have something coming up hopefully in the next few months.
