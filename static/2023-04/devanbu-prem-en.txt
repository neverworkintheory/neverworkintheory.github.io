So yeah I've been - we've been working on naturalness and bimodality thanks for the intro and that Greg for many years now.
These days every software engineer is all excited about, and stoked about, large language models and they're use in code.
Some people derisively call large language models "stochastic parrots", with some justification,
so what I'm going to talk about for the next nine minutes or so is what's going on with these stochastic parrots which write code and programmers.
So I just want to acknowledge support from DARPA - IARPA actually -  and National Science Foundation, Sandia National Labs, and the Humboldt Foundation.
So reality check: the fact is that Codex GPT-x and so on are now widely used to generate code.
How much are people using this generated code and does it actually help?
And how good is this code?
So these two questions are what I'm going to be talking about for the rest of the - of this talk.
So I'm going to essentially present four papers.
I encourage you to go look at them, they're all very interesting, very good, using a wide range of methodologies,
and these are all human subject studies.
The first one is from CMU - Carnegie-Mellon - it's a survey of 410 developers, mostly GitHub Developers.
The second one is from Harvard, it's a control study of human subjects, a small sample - the nature of control studies.
The control here is the completion engine that's built into Visual Studio called Intellicode.
All subjects are university students, that is what it is.
And so okay, now the results.
In the case of the survey from CMU,
30% of the code was generated according to GitHub developers,
this was the number they put out.
They say it helps productivity - the GitHub developers - and they said in - 74% of them said that they do a quick check of the code produced by Copilot
before they actually use it,
so they get the completion,
they do a quick check,
and then they use it.
They also complained that it wasn't very good at dealing with non-functional requirements like security, performance, and so on.
And they also complained that it was hard to control the code that was being generated.
With the students and - they said Copilot didn't help them much
and they said they produced many defects and it was hard to understand the code they produced
but regardless of all that, the student subjects liked it anyway.
So those are the two papers from universities.
Now a couple from companies.
The first one is from GitHub - this is not using Copilot, it's using their own completion engine.
The sample size is 10K and they did - they did the study using telemetry,
in other words they gathered data remotely to see how the code was being actually used.
The second study was a more regular study in some ways.
It used a triangulation - a combination of survey and telemetry,
and so they confirmed the results for one using the other.
So the results - so the Google study found that 3% of the code was generated - 3% of the code that actually was entered was generated.
About 6% of the cycle time was reduced - so the cycle time is the time between those things the programmers do -
and about 30% of the suggestions that were made by the completion engine were accepted by the users.
So a very large sample study but, you know, as as is typical in these telemetry studies you don't get much insight.
The second study is - gives you some other insight so 23% to 28% of the suggestions produced by Copilot were accepted by developers
and the acceptance rates correlate very well with self-reported productivity according to the survey.
So this is, you know, this is all quite interesting,
so, you know, I encourage you to look at those papers,
they're all out there.
The Google study was not peer-reviewed the others are.
The Google study was a blog post.
So my personal take on these code language - large code language models is,
developers like them and they use them according to these surveys.
It's not clear that they fully understand the code they're using and this is for me confirmed
both by the studies and from conversations with people, personal anecdotal conversations.
so we don't know what the personal software process is like when people use them.
I think that's - that's still an open question - I don't know what's going on.
So you know this is probably not going to surprise you but in a surprisingly short amount of time
every computer everywhere - laptops, mobile phones, toasters, microwaves, air traffic control, nuclear power plants, cruise missiles, you name it -
they're all going to be using code generated by these language models.
So this is, you know, this is, kind of, at this point, kind of inevitable.
So all right this is the scariest slide in this talk.
AI-generated code will be running everywhere and at the very basic level, you know, the question is, do these large language models generate code?
Because if they do this damn buggy code is going to be everywhere, right?
Okay, so this is the question we were interested in.
In a - we have a paper coming up at MSR - Mining Software Repositories - which will be held in Melbourne.
My student Kevin Jesse will be presenting it, he's also graduating with a PhD in language models applied to code, so please hire him.
And you can have a look, scan the QR code for the paper.
Okay, so this is what we did.
We took this data set, which is pretty ubiquitous, it's one line bug fixes from a thousand projects,
about 17,000 samples after data cleaning.
What we can do is,
we can go back in history and version control and find out when these bugs were injected by humans.
And since we know when they were introduced, we can try Copilot with the prefix of the code at the time it was introduced
and see whether Copilot produces the buggy code or the fixed code.
There's some problems with this approach but I'll talk about them in a minute.
But it does give you some kind of insights.
One thing I should say is, all the samples in the data set that we used were fixed by the time the LLMs - by the time Copilot was trained,
it was all fixed,
so Copilot is seeing the fixed code,
and so we're sort of seeing what it does when it's trained on the fixed code,
because this data set was, you know, now three years old, maybe four.
Okay, so the result is as follows.
So in about 13% of the cases Copilot Codex reproduces the fixed code.
And about twice as often it regurgitates the buggy code.
So remember it was trained on the fixed code but it reproduces the buggy code, right,
so maybe the buggy code feels more natural to the model and so it, you know, it produces that.
So now there is a lot of, like, dark matter, which is not either the buggy code or the fixed code,
it doesn't match either one exactly.
So what we did is, we took 400 samples of this and manually examined it,
and what we found is that in about 90% of the cases it's gibberish - it's neither the bug nor the fix,
it probably won't even compile,
it just produces some random stuff, right.
But some of the cases, about, you know, 5% of the cases,
it's code that actually is either the bug or the fix where it is just written in some other form that we can recognize.
In some cases we just didn't know.
Okay, so I have about three minutes left,
so let me just tell you - mention a couple of things we also looked at.
When Copilot generates this simple, stupid buggy code we looked at whether they were stickier,
because we know how long they stayed in the - in the version control history, right,
So if it stays longer in the version control history you can think of it as being sticky.
In some sense people don't actually see it, you know,
so we looked at that as well.
And the other thing we looked at is the language models.
These large language models have enormous prior spaces - they're representing the priors, the prefix, and this enormous space,
and so they can be pushed around in this space.
And one way to push them around is to, metaphorically, you - to let them behave like a good programmer - is to put comments in the - in the code, right.
So we added comments and we see whether Copilot repeats those errors when you put the comments in there.
All right, so, this is my last slide.
So the takeaways:
programmers love these plugins for good or ill.
Again, these large language models often recapitulate human errors,
and when they do,
it appears that these errors stick around for a longer period of time,
maybe because large language models - large language models produce code that looks natural.
So maybe it's harder for human eyes to see these errors
But the good news is,
we can improve their performance by adding comments in.
More details are in the paper.
So my main take continues to be that developers will use these large language models
and it may be that these mistakes made by these models seem to somehow survive human review,
and, you know, maybe these errors are stickier.
So with that I'll stop and take questions.
All right, thank you very much, Prem.
Obviously a lot of interest in this topic and that's an understatement.
One of the comments that has come in - do you believe that as we come to rely more and more on LLMs for code generation
that it's going to make it harder for new languages to gain a following,
because of course they won't have the corpus of code to train the LLMs,
so the tools that programmers will be used to using simply won't work as well,
so we'll be stuck with present-day languages.
Is - how do you feel about that?
Yeah, interesting question.
So my sense is that new languages are - are a passion project,
right,
so the people using them in the beginning and generating the largest corpora of these new languages
are not going to be affected by whether they have Copilot or not,
right,
so maybe what will happen is that once you have enough data,
then you can take one of these large language models and fine-tune them.
They are very quick at learning new things because they've been trained on so much of what human forms of expression are in languages.
If the new language is something completely different,
you know,
like APL or Haskell or something,
maybe they have a bit of a tough time,
but I think, you know, for the - in most cases I think it'll be okay.
I might be wrong it's just a prediction.
Okay, and we have a second question that's come in, and this is one that I've been wondering as well.
So if an LLM generates buggy code and it goes into production who do we blame for faults?
Do we blame the programmer who shipped the code,
do we bring blame whoever trained the model,
where does responsibility lie in this case?
Great question - I would love for the Canadian Parliament or the European Union to pass a law on this because it ain't going to happen in America.
